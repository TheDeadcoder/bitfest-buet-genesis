## Problem-1
mBART-50-many-to-many-mmt model is selected to fine-tune. 128M of 600M+ parameters are unfrozen.  notebook is added in prob 1's folder
[kaggle notebook link](https://www.kaggle.com/code/sayemshahadsoummo/kuet-bitfest-hackathon-preli)

we also tried to train t5-base model but did not get satisfactory result

## Problem-2
after running the backend as per the instructions given in the README of problem-2,
Find the api documentation here:
```bash
http://127.0.0.1:8000/docs
```
